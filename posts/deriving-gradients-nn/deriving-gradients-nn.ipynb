{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deriving the Jacobians & Gradients for Neural Networks\n",
    "\n",
    "In this article, we'll derive all the Jacobians and Gradients for a two-layer\n",
    "neural network from first principles. This will include calculating derivatives for linear layers, ReLu, Softmax\n",
    "and Cross entropy loss.\n",
    "\n",
    "## The model\n",
    "\n",
    "We'll assume a simple neural network with 1 hidden layer, similarly to the one defined within the\n",
    "[Stanford\n",
    "CS2249](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/readings/gradient-notes.pdf)\n",
    "lecture notes. In addition, we'll assume the input and output shapes correspond\n",
    "to the MNIST dataset (hand-written digits), so that these calculations can\n",
    "easily be written in code for a later post.\n",
    "\n",
    "$$ x = \\text{input} \\in \\mathbb{R}^{784 \\times 1} $$\n",
    "$$ z = Wx + b_1 \\in \\mathbb{R}^{128 \\times 1} $$\n",
    "$$ h = \\text{ReLU}(z) $$\n",
    "$$ \\theta = Uh + b_2 \\in \\mathbb{R}^{10 \\times 1} $$\n",
    "$$ \\hat{y} = \\text{softmax}(\\theta) $$\n",
    "$$ J = \\text{cross-entropy}(y, \\hat{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Pass\n",
    "\n",
    "### Jacobian of loss with respect to $\\theta$\n",
    "\n",
    "#### Derivative of softmax (with respect to $\\theta$)\n",
    "\n",
    "We begin by calculating the derivative of softmax with respect to $\\theta$. To keep notation simple, we write the sum part of softmax simply as $\\sum$:\n",
    "\n",
    "$$\n",
    "    \\hat{y}_i = \\frac{e^{{\\theta}_i}}{\\sum_{k=1}{e^{{\\theta}_k}}}\n",
    "    = \\frac{e^{{\\theta}_i}}{\\sum}\n",
    "$$\n",
    "\n",
    "Using the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule), we can\n",
    "calculate the partial derivative with respect to a specific logit ${\\theta}_j$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{\\hat{y}_i}}{\\partial{{\\theta}_j}} =\n",
    "    \\frac{{\\sum} \\times {\\frac{\\partial}{\\partial{{\\theta}_j}}e^{{\\theta}_i}}-{e^{{\\theta}_j}}{e^{{\\theta}_i}}}{{\\sum}^2}\n",
    "$$\n",
    "\n",
    "We can simplify this further, by noting that $\\frac{\\partial}{\\partial{{\\theta}_j}}e^{{\\theta}_i}$ is 1 if $i = j$ and $0$ otherwise:\n",
    "\n",
    "$$\n",
    "    \\text{If } i = j \\text{ :}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{\\hat{y}_i}}{\\partial{{\\theta}_j}} =\n",
    "    \\frac{{\\sum} \\times\n",
    "    {e^{{\\theta}_i}}-{e^{{\\theta}_i}}{e^{{\\theta}_j}}}{{\\sum}^2} =\n",
    "    \\frac{e^{{\\theta}_i}}{\\sum} \\times \\frac{\\sum - e^{{\\theta}_j}}{\\sum}\n",
    "    = \\hat{y}_i(1 - \\hat{y}_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{Else if } i \\neq j \\text{ :}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{\\hat{y}_i}}{\\partial{{\\theta}_j}} =\n",
    "    \\frac{{\\sum} \\times {0}-{e^{{\\theta}_i}}{e^{{\\theta}_j}}}{{\\sum}^2}\n",
    "    = -\\hat{y}_i \\hat{y}_j\n",
    "$$\n",
    "\n",
    "#### Derivative of loss (with respect to $\\theta$)\n",
    "\n",
    "Next we'll calculate the partial derivative of loss with respect to a specific\n",
    "logit ${\\theta}_j$:\n",
    "\n",
    "$$\n",
    "    \\text{CE}(y, \\hat{y}) = -\\sum_{i=1}^{10}{y_i\\ln(\\hat{y}_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{{\\theta}_j}} =\n",
    "    -\\sum_{i=1}{y_i \\frac{\\partial \\ln(\\hat{y}_i)}{\\partial{{\\theta}_j}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = -\\sum_{i=1}{\\frac{y_i}{\\hat{y}_i} \\frac{\\partial{\\hat{y}_i}}{\\partial{{\\theta}_j}}}\n",
    "$$\n",
    "\n",
    "Splitting this sum up (to the $i=j$ part and the $i \\neq j$ parts), we get:\n",
    "\n",
    "$$\n",
    "    = \n",
    "    - \\frac{y_j}{\\hat{y}_j} \\frac{\\partial{\\hat{y}_j}}{\\partial{{\\theta}_j}}\n",
    "    -\\sum_{i=1, i \\neq j}{\\frac{y_i}{\\hat{y}_i} \\frac{\\partial{\\hat{y}_i}}{\\partial{{\\theta}_j}}}\n",
    "$$\n",
    "\n",
    "Then we can substitute in the softmax partial derivatives for these two cases:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{{\\theta}_j}} =\n",
    "    -\\frac{y_j}{\\hat{y}_j} \\times \\hat{y}_j (1 - \\hat{y}_j)\n",
    "    - \\sum_{i=1, i \\neq j}{ \\frac{y_i}{\\hat{y}_i} \\times -(\\hat{y}_i \\hat{y}_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = -y_j + y_j \\hat{y}_j + \\sum_{i=1, i \\neq j}{y_i \\hat{y}_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\hat{y}_j \\sum_{i}{y_i} - y_j\n",
    "$$\n",
    "\n",
    "Given that y is a one-hot-encoded vector, it sums to 1.\n",
    "\n",
    "$$\n",
    "    \\sum_{i}{y_i} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{{\\theta}_j}} = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "#### Jacobian\n",
    "\n",
    "Finally this gives us the Jacobian:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{{\\theta}}} = \\hat{y} - y \\in \\mathbb{R}^{10 \\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians for the second linear layer\n",
    "\n",
    "#### Weights ($U$)\n",
    "\n",
    "The second linear layer is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\theta_j = \\sum_{l=1}{U_{k,l} \\times h_k}\n",
    "$$\n",
    "\n",
    "The gradient with respect to a specific weight $U_{k,l}$ is therefore 0 if $j\n",
    "\\neq k$ as the rules of matrix multiplication mean that $U_{k,l}$ would have no\n",
    "impact on $\\theta_i$. Otherwise it is simply $x_k$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{\\theta_j}}{\\partial{U_{k,l}}} = \n",
    "    \\begin{cases}\n",
    "        h_k & \\text{if } j = k\\\\    \n",
    "        0 & \\text{if } j \\neq k\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Then using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{U_{k,l}}} =\n",
    "    \\frac{\\partial{J}}{\\partial{\\theta}} \\times \n",
    "    \\frac{\\partial{\\theta}}{\\partial{U_{k,l}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\frac{\\partial{J}}{\\partial{\\theta}} \\times \n",
    "    \\frac{\\partial{\\theta}}{\\partial{U_{k,l}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\sum_{j=1}{\\frac{\\partial{J}}{\\partial{\\theta_j}}}\n",
    "    \\times \\frac{\\partial{\\theta_j}}{\\partial{U_{k,l}}}\n",
    "$$\n",
    "\n",
    "Since this is only non-zero when $j = k$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{U_{k,l}}} =\n",
    "    \\frac{\\partial{J}}{\\partial{\\theta_k}}\n",
    "    \\times h_l\n",
    "$$\n",
    "\n",
    "Converting this into a $\\mathbb{R}^{128 \\times 10}$ matrix (so we can simply\n",
    "subtract from $U$ for stochastic gradient descent), we arrive at the Jacobian:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{U}} =\n",
    "    \\frac{\\partial{J}}{\\partial{\\theta}}\n",
    "    \\times h^T \n",
    "    \\in \\mathbb{R}^{10 \\times l28}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias ($b_2$)\n",
    "\n",
    "As the bias is a $\\mathbb{R}^{10 \\times 1}$ vector that is simply added to\n",
    "$\\theta$, it's gradient will be the same as $\\frac{\\partial{J}}{\\partial{\\theta}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{J}}{\\partial{b_2}} =\n",
    "    \\frac{\\partial{J}}{\\partial{\\theta}}\n",
    "    \\in \\mathbb{R}^{10 \\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian for ReLU\n",
    "\n",
    "The derivative of ReLU is straight-forward:\n",
    "\n",
    "$$\n",
    "    \\text{ReLU}(z_p) = \\max(z_p, 0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{ReLU}'(z_p) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if } z_p > 0\\\\    \n",
    "        0 & \\text{if } z_p \\leq 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore using the chain rule:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{z}} =\n",
    "    \\frac{\\partial{J}}{\\partial{\\theta}}\n",
    "    \\times \\frac{\\partial{\\theta}}{\\partial{h}}\n",
    "    \\times \\frac{\\partial{h}}{\\partial{z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\theta = U h + b_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{\\theta}}{\\partial{h}} = U\n",
    "    \\in \\mathbb{R}^{10 \\times 128}\n",
    "$$\n",
    "\n",
    "To get the correct transpose:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{z}} =\n",
    "    U^T\n",
    "    \\times (\\frac{\\partial{J}}{\\partial{\\theta}})\n",
    "    \\times (\\text{ReLU}'(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians for the first linear layer\n",
    "\n",
    "Following a similar approach to the second linear layer (using the chain rule):\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{b_1}} = \\frac{\\partial{J}}{\\partial{z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{J}}{\\partial{W}} = \\frac{\\partial{J}}{\\partial{z}} \\times x\n",
    "$$\n",
    "\n",
    "And that's it! Hopefully you found this helpful, and let me know in the comments\n",
    "if you spot any issues.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
