{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks From Scratch\n",
    "## Understand how neural networks really work\n",
    "\n",
    "Ever wondered how neural networks really work under the hood?\n",
    "\n",
    "In this article, we'll build one from scratch using just basic matrix operations\n",
    "and calculus. By the end, you should understand how neural networks work, and be\n",
    "able to build one yourself from scratch.\n",
    "\n",
    "The article is written using Jupyter Lab, and you can download the corresponding\n",
    "code at\n",
    "https://github.com/alan-cooney/blog/blob/main/posts/mnist-from-scratch/mnist-from-scratch.ipynb\n",
    ". It assumes you know roughly what a Neural Network is (watch [this video](https://www.youtube.com/watch?v=aircAruvnKk&t=57s) first\n",
    "if not), and have a basic knowledge of linear algebra and calculus.\n",
    "\n",
    "## The challenge\n",
    "\n",
    "The challenge we'll use is hand-written digit recognition from the famous\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset. That is  to say, the model\n",
    "will take images of hand-written numbers, and output the number as a single integer.\n",
    "\n",
    "![MNIST Images are used for this example](images/Wikipedia-MNIST.png \"Source:\n",
    "Wikipedia\")\n",
    "\n",
    "<small>Image source: Wikipedia</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using the `tensorflow_datasets` library, as a convenient way to download the [MNIST\n",
    "dataset](http://yann.lecun.com/exdb/mnist/). This will give us two datasets -\n",
    "one for training the model, and another that should just be used at test time to\n",
    "verify it works on never-seen numbers. We'll then simply convert them to numpy\n",
    "iterators, as we won't be using TensorFlow for this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "from os import getcwd, path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Add types for the dataset download\n",
    "ds_train: tf.data.Dataset\n",
    "ds_test: tf.data.Dataset\n",
    "\n",
    "# Download\n",
    "(tf_train, tf_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'], # Split into training and testing data\n",
    "    data_dir=path.join(getcwd(), \".data\"), # Store in .data/\n",
    "    as_supervised=True, # Get tuples (features, label)\n",
    "    with_info=True, # Include extra info about the dataset\n",
    ")\n",
    "\n",
    "# Convert to Numpy iterators\n",
    "train_data: typing.Iterator[typing.Tuple[np.ndarray, int]] = tf_train.as_numpy_iterator()\n",
    "test_data: typing.Iterator[typing.Tuple[np.ndarray, int]] = tf_test.as_numpy_iterator()\n",
    "\n",
    "# Print the shape of an image\n",
    "train_data.next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach\n",
    "\n",
    "Our approach will be to create a simple one-layer neural network (NN). This\n",
    "means:\n",
    "\n",
    " 1. Each image will be converted from 28x28 grayscale pixels, to a single column vector\n",
    "   (784x1) of numbers. Each number in this vector will range from 0 (black) to\n",
    "   255 (white). The corresponding label vector ($\\bf{y}$) is the one-hot-encoded integer\n",
    "   (e.g. $1$ is $[0,1,0,...,0]$).\n",
    "\n",
    "  $$\\displaystyle \\bf{x} = \\text{image} = \n",
    "  \\begin{bmatrix} x_1 \\\\ ... \\\\ x_{784} \\end{bmatrix}$$\n",
    "\n",
    "   $$\\displaystyle \\bf{y} = \\text{one-hot encoded label} = \n",
    "  \\begin{bmatrix} y_0 \\\\ ... \\\\ y_{9} \\end{bmatrix}$$\n",
    "\n",
    "  $$\\displaystyle y_i \\in \\{0,1\\}, \\sum_{i=0}^{9}y_i = 1$$\n",
    "\n",
    " 2. We'll then initialise a matrix of random weights, of size 10 (number of\n",
    "    digits 0-9) x 784 (number of pixels).\n",
    "\n",
    "  $$\\displaystyle \\bf{w} = \\text{weights} = \n",
    "  \\begin{bmatrix} w_{0,1} & ... & w_{0,784} \\\\ ... & ...  & ... \\\\ w_{9,1} & ...& w_{9,784} \\end{bmatrix}$$\n",
    "\n",
    " 3. We'll create a simple model, that multiplies these weights by each image, to\n",
    "   produce a 10x1 set of 'activations' (outputs). There is one activation per\n",
    "   potential prediction (integers from 0-9).\n",
    "   \n",
    "$$\\displaystyle \\bf{w} * \\bf{x} = \\bf{a}  = \n",
    " \\begin{bmatrix} a_0 \\\\ .. \\\\ a_{9} \\end{bmatrix}$$\n",
    "\n",
    " 4. To convert these 10 activations into a prediction, we'll use the softmax\n",
    "    function to help pick out the biggest activation (a soft maximum). The\n",
    "    softmax (`S(a)`) function maps one vector to another (of the same shape), where all\n",
    "    the new values sum to 1. The resulting values will be the model's prediction\n",
    "    ($p$) of the probabilities that the image represents that specific integer.\n",
    " \n",
    "  $$\\displaystyle S(\\bf{a}) :\n",
    "  \\begin{bmatrix} a_0 \\\\ ... \\\\ a_{9} \\end{bmatrix}\n",
    "  \\rightarrow\n",
    "  \\begin{bmatrix} S_0 \\\\ ... \\\\ S_{9} \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\\displaystyle S_i = \\frac{e^{a_i}}{\\sum_{k=1}^{10}{e^{a_k}}}$$\n",
    "\n",
    "  $$\\displaystyle S_i = \\text{Probability that the image is the\n",
    "  integer i}$$\n",
    "\n",
    "  $$\\displaystyle \\bf{p} = \\text{predictions} = S(\\bf{a})$$\n",
    "\n",
    " 5. Next we'll calculate the loss (how inaccurate our predictions were), using\n",
    "    cross-entropy loss ($\\text{xent}(\\bf{y},\\bf{p})$). We'll use this loss function as it\n",
    "    informally gives us the distance between two probability distributions.\n",
    "    These are the correct outputs ($y$), and the predictions ($p$). This\n",
    "    completes what's known as the *forward pass*.\n",
    "\n",
    "    $$\\displaystyle \\text{xent}(\\bf{y},\\bf{p}) = -\\sum_{k=0}^{9}{y_k * \\ln{(p_k)}}$$\n",
    "\n",
    " 6. Finally, to learn, we'll calculate the gradients of the loss with\n",
    "    respect to the weights (derived later in this article), and then use\n",
    "    gradient descent (small steps) to update the weights & reduce the loss. This\n",
    "    is what's known as the *backwards pass*.\n",
    "    \n",
    " 7. Loop. We'll keep doing this, looping through each image in the dataset many\n",
    "    times. As the weights start leading to more and more accurate predictions,\n",
    "    the model can be said to be 'learning'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forwards pass\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "To begin with, we'll create the Softmax function. \n",
    "\n",
    "  $$\\displaystyle S_i = \\frac{e^{a_i}}{\\sum_{k=0}^{9}{e^{a_k}}}$$\n",
    "\n",
    "A slight complication of this is numerical stability - in that $S_i$ values can\n",
    "get close to infinity which can't be represented by a `float64` number on a\n",
    "computer (the maximum is $10^{308}$). To solve this, we use a well known trick\n",
    "whereby we multiply the numerator and denominator by a constant, typically \n",
    "$e^{-max(x)}$ which has the effect of shifting the softmax inputs closer to 0\n",
    "(without changing the output).\n",
    "\n",
    "  $$\\displaystyle S_i = \n",
    "  \\frac{e^{a_i}}{\\sum_{k=0}^{9}{e^{a_k}}}\n",
    "  =\n",
    "  \\frac{e^{a_i}*e^{-max(\\bf{a})}}{\\sum_{k=0}^{9}{e^{a_k}*e^{-max(\\bf{a})}}}\n",
    "  =\n",
    "  \\frac{e^{a_i-max(\\bf{a})}}{\\sum_{k=0}^{9}{e^{a_k-max(\\bf{a})}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def softmax(activations: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax with numerical stability\n",
    "\n",
    "    Args:\n",
    "        activations (np.ndarray): The activation values (a)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Softmax vector (S)\n",
    "    \"\"\"\n",
    "    shift_activations = activations - np.max(activations)\n",
    "    numerator = np.exp(shift_activations)\n",
    "    denominator = np.exp(shift_activations).sum()\n",
    "    return numerator/denominator\n",
    "\n",
    "# Example\n",
    "softmax([10, 10, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss\n",
    "\n",
    "Next we'll create the loss function.\n",
    "\n",
    "$$\\displaystyle \\text{xent}(\\bf{y},\\bf{p}) = -\\sum_{k=0}^{9}{y_k\\ln(p_k)}$$\n",
    "\n",
    "The reason cross-entropy loss is used so widely for classification problems, is\n",
    "that it is very simple. This is because $y_k = 0$ where $k \\neq \\text{label}$, and 1\n",
    "otherwise, so:\n",
    "\n",
    "$$\\displaystyle \\text{xent}(\\bf{y},\\bf{p}) = -\\ln(p_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def cross_entropy_loss(label: int, predictions: np.ndarray) -> float:\n",
    "    \"\"\"Cross-entropy loss\n",
    "\n",
    "    Args:\n",
    "        label (int): Label (integer value of the image)\n",
    "        predictions (np.ndarray): Prediction 10x1 matrix\n",
    "\n",
    "    Returns:\n",
    "        float: Cross-entropy loss\n",
    "    \"\"\"\n",
    "    prediction_probability_label = predictions[label]\n",
    "    return -log(prediction_probability_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward function\n",
    "\n",
    "With that done, we can create the full forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(weights: np.ndarray, image: np.ndarray, label: int) -> float:\n",
    "    \"\"\"Forward pass\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): Weights\n",
    "        image (np.ndarray): Image\n",
    "        label (int): Image label (integer from 0-9)\n",
    "\n",
    "    Returns:\n",
    "        float: Loss\n",
    "    \"\"\"\n",
    "    # First let's flatten the image (to be a vertical 784x1 matrix)\n",
    "    # We'll also normalize to be floats from 0-1 rather than ints\n",
    "    flattened_image = image.reshape(28*28,1) / 255.\n",
    "\n",
    "    # Let's also one-hot-encode the label\n",
    "    label = np.zeros(10).reshape((-1,1))\n",
    "    label.put(label, 1)\n",
    "\n",
    "    # Calculate the activations\n",
    "    activations = np.matmul(weights, flattened_image)\n",
    "\n",
    "    # Use softmax to calculate the predictions\n",
    "    predictions = softmax(activations)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = cross_entropy_loss(label, predictions)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# weights = np.random.rand(10, 28*28) / 78*78\n",
    "# data = train_data.next()\n",
    "# res = forward(weights, data[0], data[1])\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards Pass\n",
    "\n",
    "#### Chain Rule Approach\n",
    "\n",
    "Out approach here will be to use the chain rule, to calculate the derivative of\n",
    "the loss with respect to each weight:\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{\\text{loss}}}{\\partial{\\text{weight}_{i,j}}} =\n",
    "    \\frac{\\partial{\\text{loss}}}{\\partial{\\text{predictions}}} *\n",
    "    \\frac{\\partial{\\text{predictions}}}{\\partial{\\text{activations}}} * \n",
    "    \\frac{\\partial{\\text{activations}}}{\\partial{\\text{weight}_{i,j}}}$$\n",
    "\n",
    "$$\\displaystyle\n",
    "    =\n",
    "    \\frac{\\partial{\\bf{l}}}{\\partial{\\bf{p}}} *\n",
    "    \\frac{\\partial{\\bf{p}}}{\\partial{\\bf{a}}} * \n",
    "    \\frac{\\partial{\\bf{a}}}{\\partial{w_{i,j}}}$$\n",
    "\n",
    "One convenience however is that loss only depends on $p_y$ (the predicted\n",
    "probability that $x$ is integer $y$), because it is defined as follows:\n",
    "\n",
    "$$\\displaystyle \\bf{l} = \\text{xent}(\\bf{y},\\bf{p}) = -\\ln(p_y)$$\n",
    "\n",
    "This means that we can simplify our derivative calculation to:\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{\\bf{l}}}{\\partial{p_y}} *\n",
    "    \\frac{\\partial{p_y}}{\\partial{\\bf{a}}} * \n",
    "    \\frac{\\partial{\\bf{a}}}{\\partial{w_{i,j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss w.r.t. predictions\n",
    "\n",
    "Remembering that cross-entropy loss is calculated as follows:\n",
    "\n",
    "$$\\displaystyle \\bf{l} = \\text{xent}(\\bf{y},\\bf{p}) = -\\ln(p_y)$$\n",
    "\n",
    "The partial derivative is simply:\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial{\\bf{l}}}{\\partial{p_y}} = -\\frac{1}{p_y}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions w.r.t. activations\n",
    "\n",
    "The softmax for $p_y$ was defined as follows, noting how we simplify the sum\n",
    "term to be represented as just the $\\sum$ symbol:\n",
    "\n",
    "$$\\displaystyle p_y = S(a_y) = \\frac{e^{a_y}}{\\sum_{k=0}^{9}{e^{a_k}}}\n",
    "= \\frac{e^{a_y}}{\\sum}$$\n",
    "\n",
    "Using the quotient rule, for a specific element $a_j$ of $\\bf{a}$:\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{dy}{dx} = \\frac{v*\\frac{du}{dx}-u*\\frac{dv}{dx}}{v^2}\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{p_y}}{\\partial{a_i}} =\n",
    "    \\frac{{\\sum}*{\\frac{\\partial}{\\partial{a_i}}e^{a_y}}-{e^{a_y}}{e^{a_i}}}{{\\sum}^2}\n",
    "$$\n",
    "\n",
    "We can simplify this further, by noting that\n",
    "$\\frac{\\partial}{\\partial{a_i}}e^{a_y}$ is 1 if $y = i$ and $0$ otherwise:\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\text{If } y = i \\text{ :}\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{p_y}}{\\partial{a_i}} =\n",
    "    \\frac{{\\sum}*{e^{a_y}}-{e^{a_y}}{e^{a_i}}}{{\\sum}^2}\n",
    "    = p_y(1 - p_i)\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\text{Else if } y \\neq i \\text{ :}\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{p_y}}{\\partial{a_i}} =\n",
    "    \\frac{{\\sum}*{0}-{e^{a_y}}{e^{a_i}}}{{\\sum}^2}\n",
    "    = -p_y * p_i\n",
    "$$\n",
    "\n",
    "Given this, we can therefore calculate the gradient of $p_y$ with respect to all\n",
    "values of $a$:\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial{\\bf{l}}}{\\partial{\\bf{a}}} = -\\frac{1}{p_y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activations w.r.t. weights\n",
    "\n",
    "The activations were calculated as follows:\n",
    "\n",
    "$$\\displaystyle \\bf{a}  = \\bf{w} * \\bf{x}$$\n",
    "\n",
    "Given that $w_{i,j}$ is multiplied by $x_i$ (using the rules of matrix\n",
    "multiplication), we get the partial derivative as follows:\n",
    "\n",
    "$$\\displaystyle\n",
    "    \\frac{\\partial{\\bf{a}}}{\\partial{w_{i,j}}}= x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining with the chain rule\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
